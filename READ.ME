Расскажу высокоуровнево про алгоритм. Он состоит из нескольки частей:

1) Чистка данных. Все переводится в нижний регистр и из текста вычищается все, что не покрывается регуляркой a-zа-яё0-9.

2) (Опционально. Включается флагом preprocessing_tools.ENABLE_MORPHING) приведение всех слов к начальной форме. 

3) Склеиваю поля title и description

4) формируем стоблец y_train_4cat куда заносим признак принадлежности к одной из 4-х больших категорий

5) Составляем список стоп слов: для текстов, относящихся к каждой из 4-х категорий, составляем список самых употребимых слов, берем из них топы. Потом из четырех топов заносим в список стоп-слов те, которые есть хотя бы в 3-х списках = хотя бы в 3-х категориях. Если слово часто употребляется как минимум в 3-х категориях, то полезной информации он нам не принесет.

6) Обучаю классификатор, который по тексту определяет одну из 4-х категорий. В качестве классификатора использую pipeline над CountVectorizer, TfidfTransformer и LogisticRegression.

7) Внутри каждой категории нужно определить подкатегорию. Для этого обучаем 4 дополнительных классификатора. Для удобства, всю сопутствующую информацию храню в классе predict_model.

8) Для каждой категории строим confusion_matrix и видим, что некоторые пары категорий смешались, что не удивительно. Например, разделить "Для дома и дачи|Мебель и интерьер|Предметы интерьера, искусство" и Для дома и дачи|Мебель и интерьер|Другое" проблема даже для человека.

9) Найдем все такие пары категорий, где наши классификаторы "сильно ошиблись", например iPhone отнесли в категорию чехлы для iPhone (что значит сильно описано в функции calculate_treshold_for_categories_to_recalculate). Для каждой такой пары обучим отдельный классификатор, который умеет различать только эти две категории.

10) Все примененные методы дают точность в 90.57% на тесте.

Комментарии:
1) использование цены как дополнительного источника данных ни к чему успешному не привело. Предполагаемая причина в том, что добавляя к разреженной матрице (sparse matrix) полный столбец цен, мы заставляем регрессию затачиваться именно на этот столбец, что неправильно. Но, возможно, он может быть полезен для различения пар очень похожих категорий, таких как iPhone и чехлы для него.

2) Для приведения к начальное форме использую pymorphy2. Но это занимает слишком много времени. И на выборке 100к эта трансформация не закончилась в течении 3-х часов. Но, тем не менее, на меньших выборках, это действительно дает некоторый ощутимый прирост точности
