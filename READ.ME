Расскажу высокоуровнево про алгоритм. Он состоит из нескольки частей:

1) Чистка данных. Из текста вычищается все, что не покрывается регуляркой a-zа-яё0-9.

2) (Опционально. Включается флагом preprocessing_tools.ENABLE_MORPHING) приведение всех слов к начальной форме использую pymorphy2. Но это занимает слишком много времени. И на выборке 100к эта трансформация не закончилась в течении 3-х часов. На меньших выборках, это действительно дает некоторый выйгрыш точности

3) Склеиваю поля title и description

4) формируем стоблец y_train_4cat куда заносим признак принадлежности к одной из 4-х больших категорий

5) Составляем список стоп слов: для текстов, относящихся к каждой из 4-х категорий, составляем список самых употребимых слов, берем из них топы. Потом из четырех топов заносим в список стоп-слов те, которые есть хотя бы в 3-х списках = хотя бы в 3-х категориях. Если слово часто употребляется как минимум в 3-х категориях, то полезной информации он нам не принесет.

6) Обучаю классификатор, который по тексту определяет одну из 4-х категорий. В качестве классификатора использую pipeline над CountVectorizer, TfidfTransformer и LogisticRegression.

7) Внутри каждой категории нужно определить подкатегорию. Для этого обучаем 4 дополнительных классификатора. Для удобства, всю сопутствующую информацию храню в классе predict_model.

8) Для каждой категории строим confusion_matrix и видим, что некоторые пары категорий смешались, что не удивительно. Например, разделить "Для дома и дачи|Мебель и интерьер|Предметы интерьера, искусство" и Для дома и дачи|Мебель и интерьер|Другое" проблема даже для человека.

9) Найдем все такие пары категори
